---
title: "Exploration and Comments"
author: "Mirco Bazzani,Krupicz Greta"
date: "11/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readr)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(ggcorrplot)
library(caret)
library(fastDummies)
library(kableExtra)

training <- read_csv("Data/training.csv")

```


## Initial exploration

What we want to achieve is to gain an approximate regression on housing prices using descriptiv metrics of individual flats such as room numbers, floors and additional metrics based on location.

The dataset we are goint to use consists of 100 variables and 90'000 rows.


```{r summary full dataset, include=FALSE}
summary(training)

```


The dataset is way to extensive. We wil therefore kick out all Variables with a large amount of missing values.


```{r}

training %>% mutate_at(c("KTKZ", "GDENR", "floors", "quarter_general"), as.factor) %>% 
  mutate_at(c("balcony","cabletv","elevator","kids_friendly",
              "parking_indoor","parking_outside"), as.logical) %>%
  mutate(year_built = as.numeric(year_built)) %>% 
  replace_na(list(balcony = FALSE, 
                  cabletv = FALSE,
                  elevator= FALSE,
                  kids_friendly= FALSE,
                  parking_indoor= FALSE,
                  parking_outside= FALSE)) %>% 
  mutate(date = as.Date(date, format ="%d.%m.%Y")) -> training

training %>% select(rent_full,
                    GDENR, #If we want to match our dataset with external data
                    KTKZ, #Could be interesting, as flat prices vary per canton
                    area, 
                    balcony, #Small amount of missing values
                    cabletv, #Small amount of missing values
                    elevator,
                    floors,
                    msregion, #Not clear what this means
                    parking_indoor,
                    parking_outside,
                    quarter_general,
                    rooms,
                    year_built,
                    Micro_rating,
                    Micro_rating_NoiseAndEmission,
                    Micro_rating_Accessibility,
                    Micro_rating_DistrictAndArea,
                    Micro_rating_SunAndView,
                    Micro_rating_ServicesAndNature,
                    wgh_avg_sonnenklasse_per_egid,
                    Anteil_auslaend,
                    Avg_age,
                    Avg_size_household,
                    Noise_max,
                    anteil_efh,
                    apoth_pix_count_km2,
                    avg_anzhl_geschosse,
                    dist_to_4G,
                    dist_to_haltst,
                    dist_to_highway,
                    dist_to_lake,
                    dist_to_main_stat,
                    dist_to_school_1,
                    dist_to_train_stat,
                    geb_wohnnutz_total,
                    dist_to_river,
                    restaur_pix_count_km2, 
                    superm_pix_count_km2) -> training_reduced_large

training %>% select(rent_full, #If we want to match our dataset with external data
                    GDENR, #If we want to match our dataset with external data
                    KTKZ, #Could be interesting, as flat prices vary per canton
                    area, 
                    balcony, #Small amount of missing values
                    cabletv, #Small amount of missing values
                    elevator,
                    floors,
                    msregion, #Not clear what this means
                    parking_indoor,
                    parking_outside,
                    quarter_general,
                    rooms,
                    year_built,
                    Micro_rating,
                    Micro_rating_NoiseAndEmission,
                    Micro_rating_Accessibility,
                    Micro_rating_DistrictAndArea,
                    Micro_rating_SunAndView,
                    Micro_rating_ServicesAndNature
                    ) -> training_microrating

training %>% select(rent_full,
                    GDENR, #If we want to match our dataset with external data
                    KTKZ, #Could be interesting, as flat prices vary per canton
                    area, 
                    balcony, #Small amount of missing values
                    cabletv, #Small amount of missing values
                    elevator,
                    floors,
                    msregion, #Not clear what this means
                    parking_indoor,
                    parking_outside,
                    quarter_general,
                    rooms,
                    year_built,
                    Anteil_auslaend,
                    Avg_age,
                    Avg_size_household,
                    Noise_max,
                    anteil_efh,
                    apoth_pix_count_km2,
                    avg_anzhl_geschosse,
                    dist_to_4G,
                    dist_to_haltst,
                    dist_to_highway,
                    dist_to_lake,
                    dist_to_main_stat,
                    dist_to_school_1,
                    dist_to_train_stat,
                    geb_wohnnutz_total,
                    dist_to_river,
                    restaur_pix_count_km2, 
                    superm_pix_count_km2) -> training_no_microrating

```


This left us with 3 datasets.

* One includes both micro ratings and descriptive variables about the surrounding area. k = 39 variables.
* One only contains the micro ratings and data about the flat. k = 20 variables.
* One without the micro ratings in order to surpass possible covariance.


```{r}

training_reduced_large %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()

training_reduced_large %>% select(dist_to_haltst) %>% 
  mutate(dist_to_haltst = log(dist_to_haltst)) %>% 
  ggplot(aes(dist_to_haltst)) +
  geom_histogram() +
  labs(title = "Log-transformation of distance to the next bus stop")

training_reduced_large %>% 
  keep(is.factor) -> factors
  
factors %>% ggplot(aes(KTKZ)) +
  geom_bar() + 
  labs(title = "Count per canton") -> p1

factors %>% ggplot(aes(floors)) +
  geom_bar() +
  labs(title = "Count per floor") -> p2

factors %>% ggplot(aes(quarter_general)) +
  geom_bar() +
  labs(title = "Count of quarter Index")  -> p3

grid.arrange(p1,p2,p3, nrow = 2)
```

The **micro-ratings** follow an approximate normal distribution, and are therefore ok. We could use them as regressors without any hassle.

But most of the numeric values are right skewed, this includes:
area
"Avg_size_household", 
"dist_to_haltst",
"dist_to_highway",
"dist_to_lake",
"dist_to_main_stat",
"dist_to_river",
"dist_to_school_1",
"dist_to_train_stat"

A log-transformation could come in handy on order to improve the regression model.

Note: The dependent variable is skewed as well, but if we log this one too the interpretability of our model decreases, so I'm not sure if it would be a good idea.

Furthermore: the log-regression makes sense for the linear model, but I do not know if we should do that for the other aproaches as well.

```{r log transformation}
training_no_microrating %>% 
  mutate(across(c("area",
                  "Avg_size_household", 
                  "dist_to_haltst",
                  "dist_to_highway",
                  "dist_to_lake",
                  "dist_to_main_stat",
                  "dist_to_river",
                  "dist_to_school_1",
                  "dist_to_train_stat"), 
                log)) -> training_no_mr_log

training_no_mr_log %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()

```


### Linear Model

```{r No logged values}
training_no_microrating %>% select(!GDENR) -> tr_mr

lm_norm <- lm_log <- lm(data = tr_mr, rent_full ~ .)
lm_norm
summary(lm_norm)
plot(lm_norm)
```

If none of the values are logged, we get an $R^2$ of .703. This is already pretty good.

What happens if we log the rent:

```{r R Dependent is logged}

lm_log_dep <- lm_log <- lm(data = tr_mr, log(rent_full) ~ .)
summary(lm_log_dep)
plot(lm_log_dep)

```
This leaves us with an increase of $R^2$ by 3%

As a last experiment I want to log both the dependent and the independent values:

```{r Linear Log Model - both sides logged}
summary(training_no_mr_log)
training_no_mr_log %>% select(!GDENR) -> tr_no_mr_log
lm_log <- lm(data = tr_no_mr_log, log(rent_full) ~ .)
summary(lm_log)
plot(lm_log)
data.frame(summary(lm_log)$coef[summary(lm_log)$coef[,4] <= .05, 4])

```

This increases the model accuracy even further. Lets try predictions with both of the logged models.

What we can say for all of the 3 fitted models is that they are really bad at predicting both very low and really high values. I'll therefore try to delete the values that have the highest leverage on the model and see if it results in an increase of prediction quality:

```{r modell optimisation}

tr_mr = tr_mr[-c(13081,88785,69663),]
lm_log_dep_corr <- lm_log <- lm(data = tr_mr, log(rent_full) ~ .)
summary(lm_log_dep_corr)
plot(lm_log_dep_corr)
```

Indeed, our model fit increased by 1%.

The next step is a prediction based on a 80% / 20% split. I'll use a logged dependent value, and kick out some of the categorical values. The reson for this being that we don't have the same factor distribution in test- and trainingset. This creates a problem, bacause we do not have the same cantons / floors as regressors and thus can't predict the outcome.

We could surpass this by using dummy variables, but when I tried it, I only got non-significant values.

```{r prediction_lm 1}
split_size <- .8
sample_size <- floor(split_size * nrow(tr_mr))
set.seed(123)
train_ind <- sample(seq_len(nrow(tr_mr)), size = sample_size)

training <- tr_mr[train_ind,]
test <- tr_mr[-train_ind,]

trn <-  select(training, -c(KTKZ, floors))
tst <- select(test, -c(KTKZ,floors))

lm_fit <- lm(data = trn, formula = log(rent_full) ~ .)
summary(lm_fit)

tst$predict <- stats::predict(lm_fit, tst)

tst %>% mutate(predictors = exp(predict)) -> tst

tst <- tst[-7863,] #deleting outlier

ggplot(tst, aes(rent_full, predictors)) +
  geom_point()

```

We significantly lost prediction power, but it can be used as an approximate regression model.

But I want to further improve on the model. That's why I am going to take another approach, and fit a Lassoo-Regression on the Dataset that contains *both* Micro Ratings and our standard regressor. I'll keep the rent_full value logged.

Let's see where we land:

NOTE: the first lassoo model only produces errors (if we accept a Stackoverflow-investigation due to an error in the C+ code). I'll use a second model

```{r lassoo prep, eval=FALSE, include=FALSE}
library(lasso2)

training_reduced_large %>% select(!c(floors, GDENR, Micro_rating_ServicesAndNature)) -> baseset

split_size <- .8
sample_s <- floor(split_size * nrow(baseset))
set.seed(123)
train_index <- sample(seq_len(nrow(baseset)), size = sample_size)

training_large <- baseset[train_index,]
test_large <- baseset[-train_index,]

lass_bas <- summary(l1ce(rent_full ~ ., data = select_if(training_large,is.numeric)))$coef
lass_2 <- summary(l1ce(rent_full ~ ., data = select(training_large, c(rent_full,
                                                              area,
                                                              msregion,
                                                              rooms,
                                                              year_built,
                                                              Micro_rating_Accessibility,
                                                              Micro_rating_DistrictAndArea,
                                                              Anteil_auslaend,
                                                              apoth_pix_count_km2,
                                                              avg_anzhl_geschosse,
                                                              dist_to_4G,
                                                              dist_to_lake,
                                                              dist_to_river,
                                                              restaur_pix_count_km2))))$coef

lass_3 <- summary(l1ce(rent_full ~ ., data = select(training_large, c(rent_full,
                                                              area,
                                                              msregion,
                                                              rooms,
                                                              Micro_rating_DistrictAndArea,
                                                              apoth_pix_count_km2,
                                                              avg_anzhl_geschosse,
                                                              dist_to_4G,
                                                              dist_to_river,
                                                              restaur_pix_count_km2))))$coef

```

Imputation of Values in order to deal with NAs:


```{r}

training_reduced_large %>% select(!c(GDENR)) -> baseset
summary(baseset_2)
print(levels(baseset$KTKZ))

#Create a model that is computable by the new mode. It only works with integers
baseset <- baseset %>% mutate_if(is.factor, as.integer) %>% mutate_if(is.logical, as.integer)
summary(baseset)


library(mice)
#we first impute the missing values based on a multivariate imputation
test_impute <- mice(baseset, seed = 123)

test_impute$imp$floors 
test_impute$imp$Anteil_auslaend 
test_impute$imp$anteil_efh
test_impute$imp$year_built
test_impute$imp$avg_anzhl_geschosse

complete_set <- complete(test_impute,1)
summary(baseset)

summary(lm(data = baseset, formula = rent_full ~.))

summary(lm(data = complete_set, formula = rent_full ~.))

summary(complete_set)

#What we see is that an imputation works well for the aggreagte level - the coefficients (median, mean & quantile) stay the same, but the really loose
#interpretability on a row by row-level.

```
Result: the overall fit of the model got worse!


Try 2, now with lasso and ridge-comparison.

```{r}
no_na <- na.omit(baseset)

#Splitting up X and Y variables
X <- no_na %>% select(!c(rent_full))
Y <- no_na %>% select(rent_full)

#Normalizing data
processed <- preProcess(X, method = c("center", "scale"))
X <- predict(processed, X)


index <- createDataPartition(Y$rent_full, p=0.75, list=FALSE)

X_train <- X[ index, ]
X_test <- X[-index, ]
Y_train <- Y[index,]
Y_test<-Y[-index,]

lasso <- caret::train(x = X_train, 
                      y = Y_train,
                      method = "glmnet", 
                      tuneGrid = expand.grid(alpha = 1, lambda = 1))

lasso_test <- train(x = X_train[,2:37],
                    y = X_train[,1],
                    method = "glmnet", 
                    tuneGrid = expand.grid(alpha = 1, lambda = 1))

summary(X_train)
```



Let's bootstrap our model:

```{r}
library(boot)

bootReg <- function(formula, data, index){
  d <- data[index,]
  fit <- lm(formula, data = d)
  return(coef(fit))
}

subset_test <- tail(tr_mr, 300)
subset_train <- head(tr_mr, nrow(tr_mr)-300)

bootResults <- boot(statistic = bootReg, formula = log(rent_full) ~ . , data = tr_mr, R = 1000)

boot.ci(bootResults, type = "bca")

```



